{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "IawStAMhtY0q",
    "outputId": "1842da91-2feb-4f65-ebac-e642925e1454"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "SctI2LsOt3dh",
    "outputId": "b4f3394d-6829-497b-bcf0-12fe84af0bf7"
   },
   "outputs": [],
   "source": [
    "!pip install spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "xi7TOrG8tO6X",
    "outputId": "12ea5c1b-bad1-48ff-da34-c419eff0da0f"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import Sequential,layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import tensorflow_probability as tfp\n",
    "from operator import truediv\n",
    "from tensorflow.compat.v1.distributions import Bernoulli\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import spectral\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVy7URxfN6tc"
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "im_width, im_height, im_depth, im_channel = 11,11,10,1\n",
    "mc_loss_weight = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TaFcFKyBN6tf"
   },
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmFAiil_N6tf"
   },
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "  # x : (n,d)\n",
    "  # y : (m,d)\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htsJKnEz3rkh"
   },
   "outputs": [],
   "source": [
    "def _bernoulli(shape, mean):\n",
    "    return tf.nn.relu(tf.sign(mean - tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGfdaFk4KNE6"
   },
   "outputs": [],
   "source": [
    "class DropBlock3D(tf.keras.layers.Layer):\n",
    "    def __init__(self, keep_prob, block_size, scale=True, **kwargs):\n",
    "        super(DropBlock3D, self).__init__(**kwargs)\n",
    "        self.keep_prob = float(keep_prob) if isinstance(keep_prob, int) else keep_prob\n",
    "        self.block_size = int(block_size)\n",
    "        self.scale = tf.constant(scale, dtype=tf.bool) if isinstance(scale, bool) else scale\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 5\n",
    "        _, self.d, self.h, self.w, self.channel = input_shape.as_list()\n",
    "        # pad the mask\n",
    "        p0 = (self.block_size - 1) // 2\n",
    "        p1 = (self.block_size - 1) - p0\n",
    "        self.padding = [[0, 0], [p0, p1], [p0, p1], [p0, p1], [0, 0]]\n",
    "        self.set_keep_prob()\n",
    "        super(DropBlock3D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=True, **kwargs):\n",
    "        def drop():\n",
    "            mask = self._create_mask(tf.shape(inputs))\n",
    "            output = inputs * mask\n",
    "            output = tf.cond(self.scale,\n",
    "                             true_fn=lambda: output * tf.compat.v1.to_float(tf.size(mask)) / tf.reduce_sum(mask),\n",
    "                             false_fn=lambda: output)\n",
    "            return output\n",
    "\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "        output = tf.cond(tf.logical_or(tf.logical_not(training), tf.equal(self.keep_prob, 1.0)),\n",
    "                         true_fn=lambda: inputs,\n",
    "                         false_fn=drop)\n",
    "        return output\n",
    "\n",
    "    def set_keep_prob(self, keep_prob=None):\n",
    "        \"\"\"This method only supports Eager Execution\"\"\"\n",
    "        if keep_prob is not None:\n",
    "            self.keep_prob = keep_prob\n",
    "        d, w, h = tf.compat.v1.to_float(self.d), tf.compat.v1.to_float(self.w), tf.compat.v1.to_float(self.h)\n",
    "        self.gamma = ((1. - self.keep_prob) * (d * w * h) / (self.block_size ** 3) /\n",
    "                      ((d - self.block_size + 1) * (w - self.block_size + 1) * (h - self.block_size + 1)))\n",
    "\n",
    "    def _create_mask(self, input_shape):\n",
    "        sampling_mask_shape = tf.stack([input_shape[0],\n",
    "                                        self.d - self.block_size + 1,\n",
    "                                        self.h - self.block_size + 1,\n",
    "                                        self.w - self.block_size + 1,\n",
    "                                        self.channel])\n",
    "        mask = _bernoulli(sampling_mask_shape, self.gamma)\n",
    "        mask = tf.pad(mask, self.padding)\n",
    "        mask = tf.nn.max_pool3d(mask, [1, self.block_size, self.block_size, self.block_size, 1], [1, 1, 1, 1, 1], 'SAME')\n",
    "        mask = 1 - mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "215a8XhEN6ti",
    "outputId": "047f3883-874a-4ddc-aaba-ed12136991a7"
   },
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape = (im_height, im_width, im_depth, im_channel))\n",
    "out1 = layers.Conv3D(filters=8, kernel_size=(3,3,7), activation='relu',input_shape=(im_height, im_width, im_depth, im_channel),padding='same')(input_layer)\n",
    "out1 = DropBlock3D(0.7,3)(out1)\n",
    "out2 = layers.Conv3D(filters=16, kernel_size=(3,3,5), activation='relu',padding='same')(out1)\n",
    "out2 = DropBlock3D(0.7,3)(out2)\n",
    "out3 = layers.Conv3D(filters=32, kernel_size=(3,3,3), activation= 'relu')(out2)\n",
    "out3 = layers.Reshape((out3.shape[1], out3.shape[2], out3.shape[3]*out3.shape[4]))(out3)\n",
    "out3 = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(out3)\n",
    "out4 = layers.Flatten()(out3)\n",
    "out4 = layers.Dropout(0.4)(out4, training=True)\n",
    "out4 = layers.Dense(256, activation='relu')(out4)\n",
    "out5 = layers.Dropout(0.4)(out4,training=True)\n",
    "out5 = layers.Dense(128, activation='relu')(out5)\n",
    "model = Model(inputs=input_layer,outputs=out5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "colab_type": "code",
    "id": "z_F0dejAN6tl",
    "outputId": "696ec932-5536-4daa-b79f-4ace0b7a2a66"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64,to_file='Hyper_Proto.png',expand_nested=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAOZ0z1rN6to"
   },
   "outputs": [],
   "source": [
    "class Prototypical(Model):\n",
    "    def __init__(self, model, w, h, d, c):\n",
    "        super(Prototypical, self).__init__()\n",
    "        self.w, self.h, self.d, self.c = w, h, d, c\n",
    "        self.encoder = model\n",
    "\n",
    "    def call(self, support, query, support_labels, query_labels, K, C, N,n_times,training=True):\n",
    "      n_class = C                                                               #10\n",
    "      n_support = K                                                             #5\n",
    "      n_query = N                                                               #15 \n",
    "\n",
    "      if training == True : \n",
    "        loss = 0\n",
    "        mc_predictions = []                                                     # list of predictions for multiple passes\n",
    "        for i in range(n_times) :\n",
    "          y = np.zeros((int(C*N),C))                                              #(150,10)\n",
    "          for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for true labels\n",
    "            y[i][x] = 1.                                                # n_times passing every query image for calculating variance \n",
    "          cat = tf.concat([support,query], axis=0)                              # [200,9,9,20,1])   \n",
    "          z = self.encoder(cat)                                                 # [200, 320]\n",
    "          # Divide embedding into support and query\n",
    "          z_prototypes = tf.reshape(z[:n_class * n_support],[n_class, n_support, z.shape[-1]])   #[10, 5, 320])\n",
    "          # Prototypes are means of n_support examples\n",
    "          z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)              #[10, 320]\n",
    "          z_query = z[n_class * n_support:]                                     #[150, 320]                         \n",
    "          # Calculate distances between query and prototypes\n",
    "          dists = calc_euclidian_dists(z_query, z_prototypes)                   #[150, 10]\n",
    "          # log softmax of calculated distances\n",
    "          log_p_y = tf.nn.log_softmax(-dists, axis=-1)                          #[150, 10]        \n",
    "          loss1 = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y, log_p_y), axis=-1)))   #loss for the current pass                     \n",
    "          loss += loss1                                                         # adding loss for each pass                   \n",
    "          predictions = tf.nn.softmax(-dists, axis=-1)                                 # prediction probability for the search-space classes per query image(for current pass)\n",
    "          mc_predictions.append(predictions)                                          \n",
    "\n",
    "        y = np.zeros((int(C*N),C))\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for true labels\n",
    "            y[i][x] = 1. \n",
    "        mc_predictions = tf.convert_to_tensor(np.reshape(np.asarray(mc_predictions),(n_times,int(C*N),C)))  #(n_times,150,10)\n",
    "        std_predictions = tf.math.reduce_std(mc_predictions,axis=0)\n",
    "        std = tf.reduce_sum(tf.reduce_sum(tf.multiply(std_predictions,y),axis=1))\n",
    "        #print('std', std)        \n",
    "        loss += mc_loss_weight*std\n",
    "        temp_list.append(std)\n",
    "        # calculating mean accuracy\n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (150,10)\n",
    "        mean_eq = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "            tf.cast(tf.argmax(mean_predictions, axis=-1), tf.int32), \n",
    "            tf.cast(tf.argmax(y,axis=-1), tf.int32)), tf.float32)\n",
    "        mean_accuracy = tf.reduce_mean(mean_eq)\n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (150,10)\n",
    "        return loss, mean_accuracy, mean_predictions   \n",
    "      \n",
    "      if training == False :\n",
    "        loss = 0\n",
    "        mc_predictions = []                                                     # list of predictions for multiple passes  \n",
    "        for i in range(n_times) :                                               # n_times passing the query images for variance calculation\n",
    "          y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "          for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.  \n",
    "          # merge support and query to forward through encoder\n",
    "          cat = tf.concat([support,query], axis=0)                              # [200,9,9,20,1]   \n",
    "          z = self.encoder(cat)                                                 # [200, 320]\n",
    "          # Divide embedding into support and query\n",
    "          z_prototypes = tf.reshape(z[:n_class * n_support],[n_class, n_support, z.shape[-1]])   #[10, 5, 320])\n",
    "          # Prototypes are means of n_support examples\n",
    "          z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)              #[10, 320]\n",
    "          z_query = z[n_class * n_support:]                                     #[150, 320]                         \n",
    "          # Calculate distances between query and prototypes\n",
    "          dists = calc_euclidian_dists(z_query, z_prototypes)                   #[150, 10]\n",
    "          # log softmax of calculated distances\n",
    "          log_p_y = tf.nn.log_softmax(-dists, axis=-1)                          #[150, 10]        \n",
    "          loss1 = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y, log_p_y), axis=-1)))        \n",
    "          loss += loss1\n",
    "          predictions = tf.nn.softmax(-dists, axis=-1)                                 # prediction probabilities for the classes for current pass\n",
    "          mc_predictions.append(predictions)                                             \n",
    "        y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.  \n",
    "        mean_predictions = tf.reduce_mean(mc_predictions,axis=0)                # mean prediction probability for each class (150,10)\n",
    "        mean_eq = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "            tf.cast(tf.argmax(mean_predictions, axis=-1), tf.int32), \n",
    "            tf.cast(tf.argmax(y,axis=-1), tf.int32)), tf.float32)\n",
    "        mean_accuracy = tf.reduce_mean(mean_eq)\n",
    "        mean_pred_index = tf.argmax(mean_predictions,axis=1)\n",
    "        # mean class-wise accuracies\n",
    "        mean_correct_class = [[] for i in range(tC)]\n",
    "        mean_correct_pred = [[] for i in range(tC)]\n",
    "        classwise_mean_acc = [[] for i in range(tC)]\n",
    "        for i in range(int(C*N)):\n",
    "          x = support_labels.index(query_labels[i])\n",
    "          mean_correct_class[x].append('4')\n",
    "          if(mean_pred_index[i] == x) :\n",
    "            mean_correct_pred[x].append('4')\n",
    "        for i in range(tC) :\n",
    "           z = len(mean_correct_pred[i])/len(mean_correct_class[i])\n",
    "           classwise_mean_acc[i].append(z)  \n",
    "        #std calculation\n",
    "        std = 0\n",
    "        for i in range(int(C*N)) :\n",
    "           x = support_labels.index(query_labels[i])\n",
    "           p_i = np.array([p[i,:] for p in mc_predictions])\n",
    "           std_i = tf.math.reduce_std(p_i,axis=0) \n",
    "           std_i_true = std_i[x]\n",
    "           std += std_i_true                                                    # adding std of each class\n",
    "        #print('std',std)\n",
    "        loss += mc_loss_weight*std \n",
    "        y = np.zeros((int(C*N),C))                                            # (150,10)\n",
    "        for i in range(int(C*N)) :\n",
    "            x = support_labels.index(query_labels[i])                           # creation of 1-hot for the true labels\n",
    "            y[i][x] = 1.                                                                \n",
    "        return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y\n",
    "\n",
    "\n",
    "      def save(self, model_path):\n",
    "        self.encoder.save(model_path)\n",
    "\n",
    "      def load(self, model_path):\n",
    "        self.encoder(tf.zeros([1, self.w, self.h, self.c]))\n",
    "        self.encoder.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dX2P_sAOFevh"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3ipb9FLtO6c"
   },
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    data = sio.loadmat('/content/drive/My Drive/houston.mat')['houston']\n",
    "    labels = sio.loadmat('/content/drive/My Drive/houston_gt.mat')['houston_gt']\n",
    "    return data, labels\n",
    "# without reduction of 200 channels to 30 channels, memory error while creating cube \n",
    "def applyPCA(X, numComponents):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def padWithZeros(X, margin):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)  # X :(145, 145, 30) --> (195, 195, 30) with window =25\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))  # (21025, 25, 25, 30)   \n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))  # (21025,)\n",
    "    patchIndex = 0\n",
    "    \n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]  \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]            \n",
    "            patchIndex = patchIndex + 1\n",
    "  \n",
    "    patchesData = np.expand_dims(patchesData, axis=-1)\n",
    "    return patchesData,patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_iyX4iTQUOpT",
    "outputId": "0d7072ba-5a1f-48e2-a727-660729b378b4"
   },
   "outputs": [],
   "source": [
    "dataset1 = 'houston'                                         # 15 classes   \n",
    "ho_x1, ho_y = loadData(dataset1)                              \n",
    "ho_x2,pca = applyPCA(ho_x1,numComponents=im_depth)                   \n",
    "ho_X,ho_Y = createImageCubes(ho_x2, ho_y, windowSize=11)   \n",
    "print(ho_X.shape,ho_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMY1DbyVZtV1"
   },
   "outputs": [],
   "source": [
    "def patches_class(X,Y,n) :\n",
    "  n_classes = n\n",
    "  patches_list = []\n",
    "  for i in range(1,n_classes+1):   # not considering class 0\n",
    "    patchesData_Ith_Label = X[Y==i,:,:,:,:]\n",
    "    patches_list.append(patchesData_Ith_Label)\n",
    "  return patches_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBjLuxWvxMBG"
   },
   "outputs": [],
   "source": [
    "patches_class_houston = patches_class(ho_X,ho_Y,15) # class_wise list of patches #(15,) for class 0: (2009, 9, 9, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbRlaX7-o8Zu"
   },
   "outputs": [],
   "source": [
    "train_class_indices = [0,1,2,3,4,5,6,7,8]\n",
    "test_class_indices = [9,10,11,12,13,14]\n",
    "train_patches_class = [patches_class_houston[i] for i in train_class_indices]        #(9)\n",
    "test_patches_class = [patches_class_houston[i] for i in test_class_indices]        #(6) \n",
    "train_class_labels = [1,2,3,4,5,6,7,8,9]   \n",
    "test_class_labels = [10,11,12,13,14,15]    #[10...15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sL367A6HF3qf"
   },
   "source": [
    "**Prepare Training and validation Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNBNYdJX3STT"
   },
   "outputs": [],
   "source": [
    "C = 5  # n_class\n",
    "K1 = 5   # n_support\n",
    "N = 15   # n_query\n",
    "tC = 3   # classes in a test episode\n",
    "#im_height,im_width,im_depth = 11,11,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmBBYOcd7wdm"
   },
   "outputs": [],
   "source": [
    "def new_episode(patches_list,K,C,N,class_labels) :\n",
    "  selected_classes = np.random.choice(class_labels,C,replace=False)  # Randomly choice 5 Classes out of classes available\n",
    "  tsupport_patches = []\n",
    "  tquery_patches = []\n",
    "  query_labels = []\n",
    "  support_labels = list(selected_classes)\n",
    "  for x in selected_classes :\n",
    "    sran_indices = np.random.choice(len(patches_list[x-1]),K,replace=False)  # for class no X-1: select random sample no\n",
    "    support_patches = patches_list[x-1][sran_indices,:,:,:,:]\n",
    "    qran_indices = np.random.choice(len(patches_list[x-1]),N,replace=False)  # N Samples for Query\n",
    "    query_patches = patches_list[x-1][qran_indices,:,:,:,:]\n",
    "  # Support and Query patches belong to same Class \n",
    "    for i in range(N) :\n",
    "      query_labels.append(x)    # N Samples for Query\n",
    "    tquery_patches.extend(query_patches)\n",
    "    tsupport_patches.extend(support_patches)\n",
    "  temp1 = list(zip(tquery_patches, query_labels)) \n",
    "  random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "  tquery_patches, query_labels = zip(*temp1)\n",
    "  tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(C*N,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(C*K,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  return tquery_patches, tsupport_patches, query_labels, support_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZSEL1Zo3KUh"
   },
   "outputs": [],
   "source": [
    "tquery_patches, tsupport_patches, query_labels, support_labels = new_episode(patches_class_houston,K1,C,N,train_class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SyQYRKpN6t4"
   },
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtxObYGQN6t4"
   },
   "outputs": [],
   "source": [
    "ProtoModel = Prototypical(model,im_width, im_height, im_depth, im_channel)\n",
    "optimizer = tf.keras.optimizers.Adam(0.00001)          #Adam(0.001)\n",
    "n_times = 25\n",
    "std_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HvgsXRvN6t6"
   },
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "def train_step(support, query, support_labels, query_labels, K, C, N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, K, C, N,n_times,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    train_loss(loss)\n",
    "    train_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6kykozUCMjn"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/content/drive/My Drive/Research/SAMPLE_CODE_TF2_Keras/Inprogress/Hyperspectral_Classification/Training checkpoints/houston/5_shots_ckpt'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,ProtoModel = ProtoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "W_5mN_gMzLeF",
    "outputId": "3ea99ebc-d44b-4336-aca4-a49875e7cc06"
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "temp_list = []\n",
    "for epoch in range(81): # n_epochs-120 + 140 tune + 40 train + 120 tune + 60 train \n",
    "    train_loss.reset_states()  # 180\n",
    "    train_acc.reset_states()\n",
    "    \n",
    "    for epi in range(n_episodes): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels = new_episode(patches_class_houston,K1,C,N,train_class_labels)     \n",
    "        train_step(tsupport_patches, tquery_patches,support_labels, query_labels, K1, C, N)\n",
    "    template = 'Epoch {}, Episode {} Train Loss: {:.2f}, Train Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1, epi+1,train_loss.result(),train_acc.result()*100))\n",
    "    if epoch % 5 == 0 and epoch != 0 :\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9p9sGw0yAon"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQHAAuML_BF8"
   },
   "outputs": [],
   "source": [
    "def tune_episode(tune_set,tC,tK,tN,test_class_labels) :\n",
    "  selected_classes = np.random.choice(test_class_labels,tC,replace=False)\n",
    "  support_labels  = list(selected_classes)\n",
    "  query_labels = []\n",
    "  support_patches = []\n",
    "  query_patches = []\n",
    "  for x in selected_classes :\n",
    "    y = test_class_labels.index(x)\n",
    "    np.random.shuffle(tune_set[y])    \n",
    "    support_imgs = tune_set[y][:tK,:,:,:,:]    #Support 1, Query 4\n",
    "    query_imgs = tune_set[y][tK:5,:,:,:,:]\n",
    "    support_patches.extend(support_imgs)\n",
    "    query_patches.extend(query_imgs)\n",
    "    for i in range(tN) :\n",
    "      query_labels.append(x)\n",
    "  temp1 = list(zip(query_patches, query_labels)) \n",
    "  random.shuffle(temp1) \n",
    "  query_patches, query_labels = zip(*temp1)\n",
    "  query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(tC*tN,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(tC*tK,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  return query_patches, support_patches, query_labels, support_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78mUqPSj3DA6"
   },
   "outputs": [],
   "source": [
    "query_patches, support_patches, query_labels, support_labels = tune_episode(tune_set_5,3,1,4,test_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KznikxjCElAR"
   },
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "\n",
    "def tune_step(support, query, support_labels, query_labels, tK, tC, tN):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mean_accuracy, mean_predictions = ProtoModel(support, query, support_labels, query_labels, tK, tC, tN,n_times,training=True)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlIrGMXBKN8Y"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir1 = '/content/drive/My Drive/Research/SAMPLE_CODE_TF2_Keras/Inprogress/Hyperspectral_Classification/Training checkpoints/houston/5_shots_ckpt/Tune'\n",
    "checkpoint_prefix1 = os.path.join(checkpoint_dir1, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,ProtoModel = ProtoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "abc2CqQdBRFB",
    "outputId": "1d55957d-c37c-4cb9-9b1f-d1fdf10d83d4"
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "temp_list = []\n",
    "for epoch in range(41): #140 tune + 120 tune + 40 tune + 40\n",
    "    tune_loss.reset_states()  \n",
    "    tune_acc.reset_states()    \n",
    "    for epi in range(n_episodes+1): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels = tune_episode(tune_set_5,3,1,4,test_class_labels)    \n",
    "        tune_step(tsupport_patches, tquery_patches,support_labels, query_labels, 1, 3, 4)              \n",
    "    template = 'Epoch {}, Tune Loss: {:.5f}, Tune Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,tune_loss.result(),tune_acc.result()*100))\n",
    "    if (epoch+1)%10 == 0 :\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPjwIpM_tPz0"
   },
   "outputs": [],
   "source": [
    "def test_episode(test_patches_class,test_class_labels,test_C,test_K,i,f) :\n",
    "  selected_classes = test_class_labels[i:f]   # [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "  support_labels = list(selected_classes)\n",
    "  query_labels = []\n",
    "  support_patches = []\n",
    "  query_patches = []\n",
    "  for x in selected_classes :\n",
    "    y = test_class_labels.index(x)\n",
    "    support_imgs = test_patches_class[y][:test_K,:,:,:,:]\n",
    "    query_imgs = test_patches_class[y][test_K:,:,:,:,:]\n",
    "    support_patches.extend(support_imgs)\n",
    "    query_patches.extend(query_imgs)\n",
    "    for i in range(query_imgs.shape[0]) :\n",
    "      query_labels.append(x)\n",
    "  temp1 = list(zip(query_patches, query_labels)) \n",
    "  random.shuffle(temp1) \n",
    "  query_patches, query_labels = zip(*temp1)\n",
    "  x = len(query_labels)\n",
    "  query_patches = tf.convert_to_tensor(np.reshape(np.asarray(query_patches),(x,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  support_patches = tf.convert_to_tensor(np.reshape(np.asarray(support_patches),(test_C*test_K,im_height,im_width,im_depth,1)),dtype=tf.float32)\n",
    "  return query_patches, support_patches, query_labels, support_labels,x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wO2QtQRf3ZpL"
   },
   "outputs": [],
   "source": [
    "query_patches, support_patches, query_labels, support_labels,x = test_episode(test_patches_class,test_class_labels,3,5,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SP2cSUDD1H2n"
   },
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "test_loss = tf.metrics.Mean(name='test_loss')\n",
    "test_acc = tf.metrics.Mean(name='test_accuracy')\n",
    "def test_step(support, query, support_labels, query_labels, K, C, v):\n",
    "    loss, mc_predictions, mean_accuracy, classwise_mean_acc, y = ProtoModel(support, query, support_labels, query_labels, K, C, v,n_times,training=False)\n",
    "    return loss, mc_predictions, mean_accuracy, classwise_mean_acc, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "XsWfzZo5e242",
    "outputId": "a1dc6eeb-f187-424f-b8ba-86d25f49c910"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1): #1000\n",
    "    test_loss.reset_states()  \n",
    "    test_acc.reset_states()        \n",
    "    tquery_patches1, tsupport_patches1, query_labels1, support_labels1, x1 = test_episode(test_patches_class,test_class_labels,3,5,0,3)    \n",
    "    loss1, mc_predictions1, mean_accuracy1, classwise_mean_acc1, y1 = test_step(tsupport_patches1, tquery_patches1, support_labels1, query_labels1, 5, 3, x1/3)      \n",
    "    print('OA1',mean_accuracy1)\n",
    "# Class-wise Accuracy\n",
    "    for i in range(tC) :\n",
    "      print('class',i+1,classwise_mean_acc1[i])\n",
    "    print('loss',loss1)\n",
    "    tquery_patches2, tsupport_patches2, query_labels2, support_labels2, x2 = test_episode(test_patches_class,test_class_labels,3,5,3,6)    \n",
    "    loss2, mc_predictions2, mean_accuracy2, classwise_mean_acc2, y2 = test_step(tsupport_patches2, tquery_patches2, support_labels2, query_labels2, 5, 3, x2/3)\n",
    "    print('OA2',mean_accuracy2) \n",
    "# Class-wise Accuracy\n",
    "    for i in range(tC) :\n",
    "      print('class',i+4,classwise_mean_acc2[i])\n",
    "    print('loss',loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRXITw9htGuw"
   },
   "source": [
    "**Overall Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oGeG6aLFtFvM",
    "outputId": "e102c81c-79da-424b-a959-53900c38493b"
   },
   "outputs": [],
   "source": [
    "mean_predictions1 =  tf.reduce_mean(mc_predictions1,axis=0)\n",
    "mean_predictions2 =  tf.reduce_mean(mc_predictions2,axis=0)\n",
    "overall_predictions = tf.concat([mean_predictions1,mean_predictions2],axis=0)\n",
    "overall_true_labels = tf.concat([y1,y2],axis=0)\n",
    "correct_pred = tf.cast(tf.equal(                                             # accuracy for the current pass\n",
    "            tf.cast(tf.argmax(overall_predictions, axis=-1), tf.int32), \n",
    "            tf.cast(tf.argmax(overall_true_labels,axis=-1), tf.int32)), tf.float32)\n",
    "o_acc = tf.reduce_mean(correct_pred) \n",
    "print(\"Overall accuracy:\",o_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxDxvPzisig0"
   },
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "CoyQLsRUshg7",
    "outputId": "0a8ff996-c915-4c4c-af2c-b83eca7b72ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "mean_predictions1 =  tf.reduce_mean(mc_predictions1,axis=0)\n",
    "cm_pred1 = tf.argmax(mean_predictions1, axis=-1)\n",
    "mean_predictions2 =  tf.reduce_mean(mc_predictions2,axis=0)\n",
    "cm_pred2 = tf.argmax(mean_predictions2, axis=-1) + 3\n",
    "overall_predictions = tf.concat([cm_pred1,cm_pred2],axis=0)\n",
    "cm_true1 = tf.argmax(y1,axis=-1)\n",
    "cm_true2 = tf.argmax(y2,axis=-1) + 3\n",
    "overall_true_labels = tf.concat([cm_true1,cm_true2],axis=0)\n",
    "results = confusion_matrix(overall_true_labels,overall_predictions) \n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Report : ')\n",
    "print (classification_report(overall_true_labels, overall_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vps2iaJorckS"
   },
   "source": [
    "**Kappa Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kET2B52kra-7",
    "outputId": "1616bf8d-017c-4f4d-90ec-987d7d255be3"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.cohen_kappa_score(overall_true_labels,overall_predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "New_Model_Houston(128)_05_Shots_GoldenModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
